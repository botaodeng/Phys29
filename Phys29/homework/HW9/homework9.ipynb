{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px; text-align: center;\">Introduction to Computer Programming for the Physical Sciences</h1>\n",
    "<h2 style=\"font-size: 24px; text-align: center;\">Joseph F. Hennawi</h2>\n",
    "<h3 style=\"font-size: 24px; text-align: center;\">Spring 2024</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"list-style: none;\">\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Open a new Jupyter notebook</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Name your notebook with your name and Homework 1</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Open a Markdown cell at the top and write your name and Homework 1</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Open a Markdown cell before each problem and write e.g. Problem 1, Problem 2(a), etc.</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Please abide by the <b><a href=\"https://github.com/enigma-igm/Phys29/blob/main/using_AI_tools.md\">Policy and Guidelines on Using AI Tools</a></b></li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Once you finish the problems: 1) Restart the Python kernel and clear all cell outputs. 2) Rerun the notebook from start to finish so that all answers/outputs show up. 3) Save your notebook as a single .pdf file and upload it to Gradescope on Canvas by the deadline. <b>No late homeworks will be accepted except for illness accompanied by a doctor's note.</b></li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span> For parts of problems that require analytical solutions you can perform your calculations using a pencil and paper. Then  photograph your work and convert the photograph to a .pdf file using an online tool. Homework assignments can only be submitted as a single .pdf file, so you will also need to figure out how to concatenate your photo .pdf file and your notebook .pdf file into a single .pdf file that you can submit. Online websites can do this for you. Alternatively, you can code up the analytical solution to your problems in a notebook Markdown cell using the LaTeX mathematical rendering language. This is harder but a chatbot can help you learn it. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Fitting the Spectrum of an Astronomical Source\n",
    "\n",
    "The spectrum of an astronomical source the power (energy/time) it emits per unit wavelength interval\n",
    "$$\n",
    "L_\\lambda  = \\left({\\rm \\frac{energy}{time \\cdot wavelength}}\\right); \\quad \\text{units: }\\left[\\frac{\\rm J}{{\\rm s \\cdot {\\rm A}}}\\right]\\text{},\n",
    "$$\n",
    "where $\\lambda$ is the wavelength and $1{\\rm A} = 10^{-10}~{\\rm m}$ is the Angstrom unit of length. If the source is located at some distance $d$ from the Earth, we will observe an energy flux\n",
    "$$\n",
    "f_{\\lambda} = \\frac{L_{\\lambda}}{4\\pi d^2} = \\left({\\rm \\frac{energy}{time \\cdot area \\cdot wavelength}}\\right);  \\quad \\text{units: }\\left[\\frac{\\rm J}{{\\rm s \\cdot m^2 \\cdot {\\rm A}}}\\right]\\text{}.  \n",
    "$$\n",
    "(we are ignoring effects that are important at cosmological distances due to the expansion of the universe). For a telescope with an aperture of area $\\Omega$ (the area of its mirror or lens) the number of photons of wavelength $\\lambda$ that will be detected in a time interval $\\Delta t$ and a wavelength interval $\\Delta \\lambda$ is given by\n",
    "$$\n",
    "N_\\lambda = \\frac{f_{\\lambda} \\Omega \\Delta t \\Delta \\lambda}{h \\frac{c}{\\lambda}} = \\left({\\rm \\frac{energy}{photon~energy}}\\right) \\quad  \\text{units: [number of photons]}, \n",
    "$$\n",
    "where $c$ is the speed of light, $h$ is Planck's constant, and $c/\\lambda = \\nu$ is the frequency of the photons. Here $\\Delta t$ would be the exposure time used for the digial camera used to record the spectrum,  and $\\Delta \\lambda$ the wavelength interval spanned by a single spectral pixel on that camera's detector. \n",
    "\n",
    "The discrete (i.e. integer) number of photons $k_\\lambda$ landing in each wavelength bin of width $\\Delta \\lambda$ (corresponding to say a pixel on the detector) centered at $\\lambda$ will follow Poisson distribution.  That is,\n",
    "$$\n",
    "P(k_\\lambda | N_\\lambda) = \\frac{N_\\lambda^{k_\\lambda}}{k_\\lambda !} e^{-N_\\lambda},\n",
    "$$\n",
    "where $k_\\lambda$ is the discrete (i.e. integer) number of photons detected in that pixel,  $N_\\lambda$ is the true (non-integer) expected number of photon  arrivals (see the equations above), and the photon arrivals in each wavelength bin are statistically independent.  In other words the number of photons measured per pixel fluctuates about the average expected rate $N_\\lambda$ because the arrival of photons at our detector  is a *counting process*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A quasar is an extremely luminous active galactic nucleus powered by accretion onto a supermassive black hole. In this problem we will consider a hypothetical quasar spectrum consisting of two terms described by the following equation: \n",
    "$$\n",
    "N_\\lambda = N_{\\rm cont}\\left(\\frac{\\lambda}{1200~{\\rm A}}\\right)^{\\alpha} + N_{\\rm line} \\mathcal{N}(\\lambda |\\lambda_{\\rm line}, \\sigma_{\\rm line}). \n",
    "$$\n",
    "The first term describes a smooth \"power-law continuum\", which is a featureless component that spans the entire range of wavelengths. The second term describes an \"emission line\", which is a broad feature that is localized around a specific known wavelength and superposed on the smooth power-law continuum.  In the equation above,  $N_{\\rm cont}$ is the continuum amplitude, $\\alpha$ is the power law index,  $\\mathcal{N}(\\lambda |\\lambda_{\\rm line}, \\sigma_{\\rm line})$ is a Gaussian emission line profile centered on the wavelength $\\lambda_{\\rm line}=1216~{\\rm A}$, with standard deviation $\\sigma_{\\rm line} = 17~{\\rm A}$, and $N_{\\rm line}$ is the total  number of photons resulting from the emission line (i.e. the Gaussian emission line profile $\\mathcal{N}$ is normalized so its integral over wavelength is unity).  The unknown model parameters are thus $\\boldsymbol{\\theta} = (N_{\\rm cont}, \\alpha, N_{\\rm line})$. **Note the domain of these model parameters -- both $N_{\\rm cont} \\in [0,\\infty]$ and $N_{\\rm line} \\in [0,\\infty]$ because the number of photons must be positive, but $\\alpha$ can take on any real value.**\n",
    "\n",
    "Download the file [spectra.csv](https://github.com/enigma-igm/Phys29/blob/main/Phys29/homework/HW9/data/spectra.csv) at this link: https://github.com/enigma-igm/Phys29/blob/main/Phys29/homework/HW9/data/spectra.csv. This file contains three simulated quasar spectra in the form of a table with four columns: the first column contains the wavelength $({\\rm A})$, and the second, third, and fourth columns contain the integer number of photons $k_\\lambda$ detected in the given pixel for each astronomical spectrum, i.e. \n",
    "```\n",
    "wavelength, k_lam_1, k_lam_2, k_lam_3\n",
    "```\n",
    "where `k_lam_1`, `k_lam_2`, and `k_lam_3` are the three different quasar spectra. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**a)** Load the spectrum data into your notebook plot $k_\\lambda$ versus $\\lambda$ for each of the three spectra. For each spectrum guesstimate by eye the set of model parameters that you think best fits the data. Do this using the model equation for $N_\\lambda$ above and trial and error.  \n",
    "\n",
    "Now here are some instructions for making your plots. Visualizing a Poisson process in the low count regime requires some thought since the statistics are not Gaussian and so error bars corresponding to e.g. the 16th and 84th percentile credibility interval are generally asymmetric, and only become symmetric in the limit of large counts (where the Poisson distribution tends toward Gaussianity). Furthermore,  if we detect zero photons in a pixel $k_\\lambda=0$ --- a common occurrence if the rate $N_\\lambda$ is small but non-zero -- then it is unclear how to represent the error bar on such a measurement since \n",
    "$$\n",
    "\\lim_{N_\\lambda \\to 0} P(k_\\lambda | N_\\lambda) = \\begin{cases} \n",
    "1 & \\text{for } k_\\lambda=0\\\\\n",
    "0 & \\text{otherwise},  \n",
    "\\end{cases}\n",
    "$$\n",
    "so there is not really a credible interval that we can compute on our measured value of $k_\\lambda=0$, since the only possible value is  $k_\\lambda=0$.  But zero photons does not necessarily mean that the true rate $N_\\lambda=0$, it could be that we just didn't detect any photons in that pixel because of random counting fluctuations.  \n",
    "\n",
    "This is something that scientists actually spend a lot of time debating. If you are interested, you can read about the various options that have been discussed [here](https://docs.astropy.org/en/stable/api/astropy.stats.poisson_conf_interval.html). For the purposes of the current problem we will adopt the following convention. Let $k$ be the number of occurrences measured in a Poisson process (here it is the number of photons detected in a pixel, $k_\\lambda$),  the credibility interval, ${\\rm CI}$ is defined by\n",
    "$$\n",
    "{\\rm CI}(k) \\equiv (k_{16}, k_{84}) =(k + 0.5 - \\sqrt{k + 0.25}, k + 0.5 + \\sqrt{k + 0.25}).  \n",
    "$$\n",
    "In other words, we when we plot error bars, we will plot the measured value of $k$ as the central value, and the lower and upper error bars will be given by $k_{16}$ and $k_{84}$, respectively. \n",
    "This is the `pearson` option discussed [here](https://docs.astropy.org/en/stable/api/astropy.stats.poisson_conf_interval.html). \n",
    "\n",
    "\n",
    "Below I provide you with a plotting function that will make a nice plot of the spectrum with error bars following this convention. \n",
    "\n",
    "```python\n",
    "def plot_spectrum(ax, wave, k_lam): \n",
    "    \"\"\"\n",
    "    Utility function to plot a spectrum in the low photon regime. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        ax: matplotlib axis object\n",
    "            The axis to plot the spectrum on. This is a matplotlib axes object created by i.e. \n",
    "            the object oriented API of matplotlib. \n",
    "        wave: array-like\n",
    "            The wavelength array of the spectrum.\n",
    "        k_lam: array-like\n",
    "            The observed integer number of photons detected per wavelength bin.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate bin midpoints\n",
    "    bin_widths = np.diff(wave) / 2.0\n",
    "    bin_widths = np.append(bin_widths[0], bin_widths)*0.97\n",
    "    k_lam_lower = k_lam + 0.5 - np.sqrt(k_lam + 0.25)\n",
    "    k_lam_upper = k_lam + 0.5 + np.sqrt(k_lam + 0.25)\n",
    " \n",
    "    for wv, bin_width, kval, kval_lo, kval_hi in zip(wave, bin_widths, k_lam, k_lam_lower, k_lam_upper):\n",
    "        # Draw rectangles for the error bars\n",
    "        ax.fill_betweenx([kval_lo, kval_hi], wv-bin_width, wv+bin_width, color='dodgerblue', alpha=0.5)\n",
    "        # Draw horizontal lines for the measured value\n",
    "        ax.hlines(kval, wv-bin_width, wv+bin_width, color='black', linestyle='-')\n",
    "    \n",
    "```\n",
    "\n",
    "Here is an example of its usage: \n",
    "    \n",
    "```python\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "plot_spectrum(ax, wave, k_lam_1)\n",
    "# This overplots a model with wavelength bins aligned with bins of the measurements. \n",
    "ax.plot(wave, N_lam_1, color='red', drawstyle='steps-mid', label='a model spectrum')\n",
    "ax.axvline(1216, color='magenta', linestyle=':', label='Emission Line Wavelength')\n",
    "ax.legend()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**)  Note this somewhat subjective choice for how to plot error bars discussed in part **a)** **only impacts our ability to visualize the data**, but not any of our results.  As statistically sophisticated physicists, we know that we need to write down the correct likelihood function which is the full generative probabilistic model for our data, which will not depend on how we choose to plot error bars.  \n",
    "\n",
    "Write a python function (see the docstring below) that computes the natural log of the likelihood of the data given the mdoel, \n",
    "$\\ln L(k_\\lambda | \\boldsymbol{\\theta})$ (see the docstring below).  **Note that these spectra are noisy and the noise is Poissonian, not Gaussian. If you assume Gaussian distributions in this problem, you will get everything on this problem wrong!**\n",
    "\n",
    "\n",
    "```python\n",
    "def lnL(theta, wave, k_lam):\n",
    "    \"\"\"\n",
    "    The log-likelihood function for the spectrum.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        theta: array-like\n",
    "            The model parameters.  \n",
    "        wave: array-like\n",
    "            The wavelength array of the spectrum.\n",
    "        k_lam: array-like\n",
    "            The observed number of photons detected per wavelength bin.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        lnL: float\n",
    "            The log-likelihood of the model given the data.\n",
    "    \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**) Use the `scipy.optimize.minimize` function to find the maximum likelihood estimate of the model parameters, $\\boldsymbol{\\theta}$, for each of the three spectra.  Use your guesstimated model parameter values from part **a)** as the initial guess for each minimization. I suggest using the `bounds` option to enforce the physical constraints on the domain of the model parameters. Otherwise the optimizer could step you into a region of parameter space where your likelihood will evaluate to `np.nan`.  \n",
    "\n",
    "The true model parameters are in the file [true_params.csv](https://github.com/enigma-igm/Phys29/blob/main/Phys29/homework/HW9/data/true_params.csv) at this link: https://github.com/enigma-igm/Phys29/blob/main/Phys29/homework/HW9/data/true_params.csv.\n",
    "For each of the three spectra, print a comparison of your maximum likelihood estimated $\\boldsymbol{\\theta}_{\\rm maxL}$ and the true parameters, $\\boldsymbol{\\theta}_{\\rm true}$ to the screen. \n",
    "\n",
    "Make a plot of each of the three spectra showing: \n",
    "- the spectrum data. \n",
    "- the maximum likelihood model overplotted\n",
    "- the true model overplotted \n",
    "\n",
    "See the plotting example above for how to overplot models on the spectrum data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Now for the rest of the problem, assume that the $N_{\\rm cont}$ and $\\alpha$ are known perfectly to be the true values in the [true_params.csv](https://github.com/enigma-igm/Phys29/blob/main/Phys29/homework/HW9/data/true_params.csv) file, and that the only unknown parameter is $N_{\\rm line}$, which we want to infer. \n",
    "\n",
    "Redo the `scipy.optimize.minimize` procedure from part **c)** but now only varying the single parameter $N_{\\rm line}$ and keeping $N_{\\rm cont}$ and $\\alpha$ fixed at their true values.  For each of the three spectra, print a comparison of your maximum likelihood estimated $N_{\\rm line, maxL}$ and the true $N_{\\rm line, true}$ to the screen. Remake the spectrum plots from part **c)**, but now overplotting the single parameter maximum likelhood models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** For each of the three spectra, use Bayesian inference to determine the posterior distribution $P(N_{\\rm line} | k_\\lambda)$ for $N_{\\rm line}$ given the data and the known true values of $N_{\\rm cont}$ and $\\alpha$. For your prior, $Pr(N_{\\rm line})$, just assume that $N_{\\rm line}$ is uniformly distributed over all positive values, i.e. $N_{\\rm line}$ cannot be negative. For each of the three posterior distributions, compute the 16th, 50th, and 84th percentiles and print them to the screen.\n",
    "\n",
    "For each spectrum, make a two panel plot summarizing the results of the inference analogous to the figure we made in the Week9 lecture notes. Your figure should have the following features: \n",
    "- Plot the posterior  $P(N_{\\rm line} | k_\\lambda)$ in the lower panel and the cumulative distribution function, ${\\rm CDF}(\\le N_{\\rm line})$ in the upper panel. \n",
    "- Shade the 16th to 84th percentile region of $N_{\\rm line}$ gray below both curves in both panels. \n",
    "- Indicate the 50th percentile (i.e. median) value of $N_{\\rm line}$ with a vertical dashed line in both panels.\n",
    "- Indicate the true value of the parameter $N_{\\rm line}$ with a vertical dashed line in both panels.\n",
    "- Indicate the maximum likelihood estimate of $N_{\\rm line}$ with a vertical dashed line in both panels (note that you should use your answers from the single parameter optimizations in part **d)** for this, not the multiparameter optimizations of part **c)**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** For each spectrum, generate 10000 samples from the posterior distribution $P(N_{\\rm line} | k_\\lambda)$. Plot a histogram of your samples and overplot the posterior distribution $P(N_{\\rm line} | k_\\lambda)$ on top of the histogram to verify that your sampling is working correctly. Make sure you choose the histogram bin widths and the range of your plots carefully.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g)** For each spectrum, make a plot zoomed into the region around the emission line, i.e. in the range  $1165~{\\rm A} < \\lambda < 1267~{\\rm A}$. On your figure for each spectrum:\n",
    "- plot the spectrum \n",
    "- overplot 100 realizations of the model $N_\\lambda(N_{\\rm line})$ using 100 of the random samples from the posterior distribution $P(N_{\\rm line} | k_\\lambda)$ that you generated in part **f)**. Show these as semi-transparent gray lines. \n",
    "- overplot the true model.\n",
    "- overplot the maximum likelihood model, $N_\\lambda(N_{\\rm line, maxL})$, from part **d)**. (Again, we want the single parameter maximum likelihood model, not the multiparameter maximum likelihood model from part **c)**).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
