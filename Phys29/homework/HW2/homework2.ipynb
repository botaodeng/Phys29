{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px; text-align: center;\">Introduction to Computer Programming for the Physical Sciences</h1>\n",
    "<h2 style=\"font-size: 24px; text-align: center;\">Joseph F. Hennawi</h2>\n",
    "<h3 style=\"font-size: 24px; text-align: center;\">Spring 2024</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"list-style: none;\">\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Open a new Jupyter notebook</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Name your notebook with your name and Homework 1</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Open a Markdown cell at the top and write your name and Homework 1</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Open a Markdown cell before each problem and write e.g. Problem 1, Problem 2(a), etc.</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Please abide by the <b><a href=\"https://github.com/enigma-igm/Phys29/blob/main/using_AI_tools.md\">Policy and Guidelines on Using AI Tools</a></b></li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Once you finish the problems: 1) Restart the Python kernel and clear all cell outputs. 2) Rerun the notebook from start to finish so that all answers/outputs show up. 3) Save your notebook as a single .pdf file and upload it to Gradescope on Canvas by the deadline. <b>No late homeworks will be accepted except for illness accompanied by a doctor's note.</b></li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span> For parts of problems that require analytical solutions you can perform your calculations using a pencil and paper. Then  photograph your work and convert the photograph to a .pdf file using an online tool. Homework assignments can only be submitted as a single .pdf file, so you will also need to figure out how to concatenate your photo .pdf file and your notebook .pdf file into a single .pdf file that you can submit. Online websites can do this for you. Alternatively, you can code up the analytical solution to your problems in a notebook Markdown cell using the LaTeX mathematical rendering language. This is harder but a chatbot can help you learn it. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Fun with Conditional Statments\n",
    "Using ${\\tt if}, {\\tt elif}, {\\tt else}$ statements, write a Python function that takes any three distinct real input numbers $a$, $b$, and $c$, and returns the same values in a tuple in order of smallest to largest. For example, if $a=3$, $b=1$, and $c=2$, then the function should return the tuple $(1,2,3)$.  If $a=3$, $b=2$, and $c=3$, then the function should return the tuple $(2,3,3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Your solution here. \n",
    "def CompareNumber(a,b,c):\n",
    "    if(a>b):\n",
    "        i=a\n",
    "        a=b\n",
    "        b=i\n",
    "    # if a>b switch a and b\n",
    "    if(a>c):\n",
    "        i=a\n",
    "        a=c\n",
    "        c=i\n",
    "    # if a>c switch a and c\n",
    "    if(b>c):\n",
    "        i=b\n",
    "        b=c\n",
    "        c=i\n",
    "    #if b>c swith b and c\n",
    "    return a,b,c\n",
    "\n",
    "print(CompareNumber(2,3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Machine Epsilon\n",
    "In the lecture we discussed the limited precision of floating point numbers and floating point arithmetic. An important value to quantity is floating-point accuracy which is referred to as the *machine epsilon*. Please read this [Wikipedia article on the machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon) to learn more about this important concept. \n",
    "\n",
    "The machine epsilon is defined as the smallest number $\\epsilon_m$ such that $1 + \\epsilon_m > 1$. According to the Wikipedia article, the machine epsilon in python can be estimated to within a factor of two via the algorithm:\n",
    "```python\n",
    "epsilon_m = 1.0\n",
    "while (1.0 + 0.5*epsilon_m) != 1.0:\n",
    "    epsilon_m /= 2.0\n",
    "```\n",
    "\n",
    "**a)** Write a python function that implements this algorithm and returns the machine epsilon. Which float-type is used in Python (see the table of the Wikipedia article)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**b)** In lecture it was argued that in Python the smallest number that can be represented in python is about `1e-308`, which is many orders of magnitude smaller than the \\( $\\epsilon_m$ \\) that you just derived. What is the difference between the smallest representable floating point number and the machine epsilon?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**c)** Consider 32bit (binary32) floating point numbers, or so called single-precision. To within an order of magnitude estimate the machine epsilon, the smallest number that can be represented, and the largest number that can be represented. Repeat your estimates for 16bit (binary16) floating point numbers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "# Your solution here. \n",
    "# a) shows as following:\n",
    "def FindEpsilon():\n",
    "    epsilon = 1.0\n",
    "    # set the initial value of epsilon\n",
    "    while(float(1.0 + 0.5*epsilon) != float(1.0)):\n",
    "        #use the float() to make sure that they are use float to compare\n",
    "        #will not stop until the epsilon is found\n",
    "        epsilon = epsilon*0.5\n",
    "        #reduce the epsilon if it is not the one we need\n",
    "    return epsilon\n",
    "    # return the value of epsilon we find\n",
    "\n",
    "print(FindEpsilon())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "It is impossible for every variable to have the max accuracy. If doing so, it may take up too much space in ram, which is not necessary.\n",
    "\n",
    "#### c)\n",
    "for binary32 the machine epsilon should be the squre root of binary 64, which should be approximately 1.19e-7. The smallest number should be -3.4028234664e38 and largestest should be 3.4028234664e38\n",
    "for binary16 the machine epsilon should be the square root of binary 32, which should be approximately 9.773e-4. The smallest number should be -65504, and largest should be 65504\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Numerical Derivatives\n",
    "In this problem we will explore the accuracy of numerical derivatives. Consider the function $f(x) = x^2(x-1)$\n",
    "\n",
    "**a)** Analytically compute $f^\\prime(x)$ and evauate it at $x=1.0$.\n",
    "\n",
    "**b)** Write a python function that estimates the derivative of $f(x)$ numerically using the forward difference formula:\n",
    "$$\n",
    "f^\\prime(x) \\approx \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "where $h$ is a small number.\n",
    "**c)** Write another python function that estimates the derivative of $f(x)$ numerically using the symmetric difference formula:\n",
    "$$\n",
    "f^\\prime(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\n",
    "$$\n",
    "**d)** Calculate $f^\\prime(1.0)$ using your two numerical derivative functions for $h=10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, ....$ until something *really bad happens* (see below). Print out both $h$ and $f^\\prime(1.0)$, as $h$ becomes smaller and smaller. Format the output of $f^\\prime(1.0)$ to show 16 digits after the decimal point. Do the calculation using the built-in python float data type (nothing fancy please!)\n",
    "\n",
    "**e)** Based on your outputs from above, you should see that the symmetric difference formula is always more precise at a given value of $h$. To understand why this is the case we need a bit of calculcus. The Taylor expansion of $f(x)$ around $x$ is given by:\n",
    "$$\n",
    "f(x+h) = f(x) + h f^\\prime(x) + \\frac{h^2}{2} f^{\\prime\\prime}(x) + \\frac{h^3}{3!} f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!} f^{\\prime\\prime\\prime\\prime}(x) + \\cdots\n",
    "$$\n",
    "which states that for small values of $h$, the function can be expanded as a sum of powers of $h$ and higher order derivatives of $f(x)$. \n",
    "\n",
    "Derive expressions for $f^\\prime(x)$ using the Taylor expansion above for the two numerical derivative formulas that we employed in part (b) and (c).\n",
    "\n",
    "**f)** Based on your answers to part (e), explain why the symmetric difference formula is always more precise than the forward difference formula in the limit $h\\rightarrow 0$. Note that the amount of computational work for both of the derivative estimators is the same, i.e. the function is evaluated at two locations, and then division by $h$ or $2h$ is performed. Hence, this problem illustrates that numerical derivatives should always be calculated  using symmetric differences whenever possible. \n",
    "\n",
    "**g)** As $h$ becomes *too small* the precision of both of the derivative estimators starts to degrade. This is because when $h$ is extremely small, taking the difference of $f(x+h)$ and $f(x - h)$ (or $f(x+h)$ and $f(x)$) becomes problematic as you are subtracting two numbers that are very close to each other. Read this Wikipedia article on [catastrophic cancellation](https://en.wikipedia.org/wiki/Catastrophic_cancellation) and describe in your own words why the numerical preision degrades when $h$ becomes too small.\n",
    "\n",
    "It can be shown that catastrophic cancellation starts to degrade results when $h\\approx x\\sqrt{\\epsilon_m}$ (forward difference estimator) or \n",
    "$h\\approx x\\epsilon_m^{2/3}$ (symmetric difference estimator), where $\\epsilon_m\\simeq 2\\times 10^{-16}$ is the machine epsilon that we derived in Problem 3. These formulas are reliable provided that $x$ is not too close to zero (in our problem $x=1$ so we are okay). For more background on where these scalings come from see Chapter 5.7 of [Numerical Recipes](http://tinyurl.com/yc36ac7z)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "$f'(x)=3x^2-2x$\n",
    "\n",
    "$f'(1.0)=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4.0 2.0\n",
      "0.1 1.2100000000000022 1.010000000000001\n",
      "0.01 1.0201000000000127 1.0001000000000038\n",
      "0.001 1.0020010000000301 1.0000010000000281\n",
      "0.0001 1.0002000099995634 1.0000000100002238\n",
      "1e-05 1.0000200000970239 1.000000000095369\n",
      "1e-06 1.00000199987349 0.9999999999177334\n",
      "1e-07 1.0000002004240116 1.0000000000287557\n",
      "1e-08 1.0000000161269895 0.9999999994736442\n",
      "1e-09 1.000000082740371 1.0000000272292198\n",
      "1e-10 1.000000082740371 1.000000082740371\n",
      "1e-11 1.000000082740371 1.000000082740371\n",
      "1e-12 1.000088900582341 1.0000333894311098\n",
      "1e-13 0.9992007221626409 0.9997558336749535\n",
      "1e-14 0.9992007221626409 0.9992007221626409\n",
      "1e-15 1.1102230246251565 1.0547118733938987\n"
     ]
    }
   ],
   "source": [
    "# Your solution here please\n",
    "# b) is shown as below\n",
    "def fx(x):\n",
    "    return float(x**3-x**2)\n",
    "# write a function of f(x), to reduce work.\n",
    "def dfdxWithForward(x,h):\n",
    "    return float((fx(x+h)-fx(x))/(h))\n",
    "# use the formula given to return the value we need\n",
    "# c) is shown as below\n",
    "def dfdxWtihSymmetric(x,h):\n",
    "    return float((fx(x+h)-fx(x-h))/(2*h))\n",
    "# do as the instruction asks\n",
    "# d) shown as below\n",
    "for i in range(16):\n",
    "    # I choose 15 at max just because it looks really bad since h = 1e-16\n",
    "    h = 10**(-i)\n",
    "    print(h,dfdxWithForward(1.0,h),dfdxWtihSymmetric(1.0,h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e)\n",
    "\n",
    "$$\n",
    "f(x+h) = f(x) + h f^\\prime(x) + \\frac{h^2}{2} f^{\\prime\\prime}(x) + \\frac{h^3}{3!} f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!} f^{\\prime\\prime\\prime\\prime}(x) + \\cdots\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(x-h) = f(x) + (-h) f^\\prime(x) + \\frac{(-h)^2}{2} f^{\\prime\\prime}(x) + \\frac{(-h)^3}{3!} f^{\\prime\\prime\\prime}(x) + \\frac{(-h)^4}{4!} f^{\\prime\\prime\\prime\\prime}(x) + \\cdots\n",
    "$$\n",
    "\n",
    "for the method shows in b)\n",
    "$$\n",
    "f^{\\prime}(x) \\approx f^{\\prime}_{(b)}(x) = \\frac{f(x+h) - f(x)}{h}\n",
    "\n",
    "= \\frac{h f^\\prime(x) + \\frac{h^2}{2} f^{\\prime\\prime}(x) + \\frac{h^3}{3!} f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!} f^{\\prime\\prime\\prime\\prime}(x) + \\cdots}{h}\n",
    "\n",
    "= f^\\prime(x) + \\frac{h}{2} f^{\\prime\\prime}(x) + \\frac{h^2}{3!} f^{\\prime\\prime\\prime}(x) + \\frac{h^3}{4!} f^{\\prime\\prime\\prime\\prime}(x) + \\cdots\n",
    "$$\n",
    "\n",
    "for the method shows in c)\n",
    "$$\n",
    "f^{\\prime}(x) \\approx f^{\\prime}_{(c)}(x) = \\frac{f(x+h)-f(x-h)}{2h}\n",
    "\n",
    "= \\frac{2h f^\\prime(x)  + \\frac{2h^3}{3!} f^{\\prime\\prime\\prime}(x) + \\frac{2h^5}{5!} f^{\\prime\\prime\\prime\\prime\\prime}(x) + \\cdots}{2h}\n",
    "\n",
    "= f^\\prime(x)  + \\frac{h^2}{3!} f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{5!} f^{\\prime\\prime\\prime\\prime\\prime}(x) + \\cdots\n",
    "$$\n",
    "\n",
    "#### f)\n",
    "Thus, we can find the difference between the approximation in order to find out which has more factors.\n",
    "\n",
    "$$\n",
    "f^{\\prime}_{(b)} - f^{\\prime}_{(c)}\n",
    "= \\frac{h}{2} f^{\\prime\\prime}(x) + \\frac{h^3}{4!} f^{\\prime\\prime\\prime\\prime}(x) + \\cdots\n",
    "$$\n",
    "\n",
    "Thus, we can find out that $f^{\\prime}_{(b)}$ has more factors. And it is reasonalbe to state that since $f^{\\prime}_{(b)}$ has more facotrs than $f^{\\prime}_{(c)}$, $f^{\\prime}_{(c)}$ is a better approximation compared to $f^{\\prime}_{(b)}$.\n",
    "\n",
    "#### g)\n",
    "Since all the values stored in computer is not the acutual value, it is always an approximation with precision of $\\epsilon_m$. Thus, when h is very small, the f(x) and f(x+h) will have such a small difference that the result may not be trusted. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
